<!DOCTYPE html>
<html>
<head>
<title>QwenImageInpaintPipeline__call__方法详解.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="qwenimageinpaintpipeline-call-%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3">QwenImageInpaintPipeline <code>__call__</code> 方法详解</h1>
<h2 id="%E6%A6%82%E8%BF%B0">概述</h2>
<p><code>QwenImageInpaintPipeline</code> 是基于 Qwen 多模态大模型的图像修复管道，其 <code>__call__</code> 方法是执行图像修复的核心接口。该方法接受原始图像、掩码图像和文本提示，通过扩散模型在指定区域生成新的内容。</p>
<h2 id="%E6%96%B9%E6%B3%95%E7%AD%BE%E5%90%8D">方法签名</h2>
<pre class="hljs"><code><div><span class="hljs-meta">@torch.no_grad()</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(
    self,
    prompt: Union[str, List[str]] = None,
    negative_prompt: Union[str, List[str]] = None,
    true_cfg_scale: float = <span class="hljs-number">4.0</span>,
    image: PipelineImageInput = None,
    mask_image: PipelineImageInput = None,
    masked_image_latents: PipelineImageInput = None,
    height: Optional[int] = None,
    width: Optional[int] = None,
    padding_mask_crop: Optional[int] = None,
    strength: float = <span class="hljs-number">0.6</span>,
    num_inference_steps: int = <span class="hljs-number">50</span>,
    sigmas: Optional[List[float]] = None,
    guidance_scale: float = <span class="hljs-number">1.0</span>,
    num_images_per_prompt: int = <span class="hljs-number">1</span>,
    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
    latents: Optional[torch.Tensor] = None,
    prompt_embeds: Optional[torch.Tensor] = None,
    prompt_embeds_mask: Optional[torch.Tensor] = None,
    negative_prompt_embeds: Optional[torch.Tensor] = None,
    negative_prompt_embeds_mask: Optional[torch.Tensor] = None,
    output_type: Optional[str] = <span class="hljs-string">"pil"</span>,
    return_dict: bool = True,
    attention_kwargs: Optional[Dict[str, Any]] = None,
    callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
    callback_on_step_end_tensor_inputs: List[str] = [<span class="hljs-string">"latents"</span>],
    max_sequence_length: int = <span class="hljs-number">512</span>,
)</span> -&gt; Union[QwenImagePipelineOutput, Tuple]:</span>
</div></code></pre>
<h2 id="%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3">参数详解</h2>
<h3 id="%E5%BF%85%E9%9C%80%E5%8F%82%E6%95%B0">必需参数</h3>
<ul>
<li><strong><code>prompt</code></strong> (<code>str</code> 或 <code>List[str]</code>): 文本提示，描述要在掩码区域生成的内容</li>
<li><strong><code>image</code></strong> (<code>PipelineImageInput</code>): 原始图像，需要修复的图像</li>
<li><strong><code>mask_image</code></strong> (<code>PipelineImageInput</code>): 掩码图像，白色区域将被修复</li>
</ul>
<h3 id="%E4%BF%AE%E5%A4%8D%E7%89%B9%E6%9C%89%E5%8F%82%E6%95%B0">修复特有参数</h3>
<ul>
<li><strong><code>masked_image_latents</code></strong> (<code>PipelineImageInput</code>, 可选): 预计算的掩码图像潜在表示</li>
<li><strong><code>padding_mask_crop</code></strong> (<code>int</code>, 可选): 掩码裁剪的填充像素数</li>
<li><strong><code>strength</code></strong> (<code>float</code>, 默认 0.6): 修复强度，控制对原图的保留程度</li>
</ul>
<h3 id="%E6%8E%A7%E5%88%B6%E5%8F%82%E6%95%B0">控制参数</h3>
<ul>
<li><strong><code>negative_prompt</code></strong> (<code>str</code> 或 <code>List[str]</code>, 可选): 负面提示</li>
<li><strong><code>true_cfg_scale</code></strong> (<code>float</code>, 默认 4.0): 真实 CFG 缩放因子</li>
<li><strong><code>guidance_scale</code></strong> (<code>float</code>, 默认 1.0): 引导缩放因子</li>
<li><strong><code>num_inference_steps</code></strong> (<code>int</code>, 默认 50): 推理步数</li>
</ul>
<h3 id="%E5%B0%BA%E5%AF%B8%E5%8F%82%E6%95%B0">尺寸参数</h3>
<ul>
<li><strong><code>height</code></strong> (<code>int</code>, 可选): 输出图像高度</li>
<li><strong><code>width</code></strong> (<code>int</code>, 可选): 输出图像宽度</li>
</ul>
<h3 id="%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6">生成控制</h3>
<ul>
<li><strong><code>num_images_per_prompt</code></strong> (<code>int</code>, 默认 1): 每个提示生成的图像数量</li>
<li><strong><code>generator</code></strong> (<code>torch.Generator</code>, 可选): 随机数生成器</li>
<li><strong><code>sigmas</code></strong> (<code>List[float]</code>, 可选): 自定义噪声调度参数</li>
</ul>
<h3 id="%E9%AB%98%E7%BA%A7%E5%8F%82%E6%95%B0">高级参数</h3>
<ul>
<li><strong><code>latents</code></strong> (<code>torch.Tensor</code>, 可选): 预计算的潜在表示</li>
<li><strong><code>prompt_embeds</code></strong> (<code>torch.Tensor</code>, 可选): 预计算的提示嵌入</li>
<li><strong><code>prompt_embeds_mask</code></strong> (<code>torch.Tensor</code>, 可选): 提示嵌入的掩码</li>
<li><strong><code>negative_prompt_embeds</code></strong> (<code>torch.Tensor</code>, 可选): 负面提示嵌入</li>
<li><strong><code>negative_prompt_embeds_mask</code></strong> (<code>torch.Tensor</code>, 可选): 负面提示嵌入掩码</li>
</ul>
<h3 id="%E8%BE%93%E5%87%BA%E6%8E%A7%E5%88%B6">输出控制</h3>
<ul>
<li><strong><code>output_type</code></strong> (<code>str</code>, 默认 &quot;pil&quot;): 输出格式</li>
<li><strong><code>return_dict</code></strong> (<code>bool</code>, 默认 True): 是否返回字典格式结果</li>
</ul>
<h3 id="%E5%9B%9E%E8%B0%83%E5%92%8C%E8%B0%83%E8%AF%95">回调和调试</h3>
<ul>
<li><strong><code>callback_on_step_end</code></strong> (<code>Callable</code>, 可选): 每步结束时的回调函数</li>
<li><strong><code>callback_on_step_end_tensor_inputs</code></strong> (<code>List[str]</code>): 传递给回调函数的张量名称</li>
<li><strong><code>attention_kwargs</code></strong> (<code>Dict</code>, 可选): 注意力机制的额外参数</li>
<li><strong><code>max_sequence_length</code></strong> (<code>int</code>, 默认 512): 最大序列长度</li>
</ul>
<h2 id="%E8%BF%94%E5%9B%9E%E5%80%BC">返回值</h2>
<h3 id="qwenimagepipelineoutput">QwenImagePipelineOutput</h3>
<p>当 <code>return_dict=True</code> 时返回 <code>QwenImagePipelineOutput</code> 对象：</p>
<pre class="hljs"><code><div><span class="hljs-meta">@dataclass</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QwenImagePipelineOutput</span><span class="hljs-params">(BaseOutput)</span>:</span>
    images: Union[List[PIL.Image.Image], np.ndarray]
</div></code></pre>
<h3 id="tuple">Tuple</h3>
<p>当 <code>return_dict=False</code> 时返回元组：</p>
<pre class="hljs"><code><div>(images,)  <span class="hljs-comment"># 第一个元素是图像列表</span>
</div></code></pre>
<h2 id="%E5%86%85%E9%83%A8%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B">内部处理流程</h2>
<pre><code class="language-mermaid"><div class="mermaid">flowchart TD
    A[开始: __call__ 方法调用] --> B[1. 输入预处理]
    B --> B1[验证输入参数]
    B1 --> B2[处理掩码裁剪参数]
    B2 --> B3[设置批次大小]
    
    B3 --> C[2. 图像预处理]
    C --> C1[预处理原始图像]
    C1 --> C2[预处理掩码图像]
    C2 --> C3[创建掩码图像]
    C3 --> C4{是否有预计算掩码潜在表示?}
    C4 -->|否| C5[计算掩码图像潜在表示]
    C4 -->|是| D[3. 文本编码]
    C5 --> D
    
    D --> D1[编码正面提示]
    D1 --> D2{是否有负面提示?}
    D2 -->|是| D3[编码负面提示]
    D2 -->|否| E[4. 潜在空间准备]
    D3 --> E
    
    E --> E1[编码原始图像到潜在空间]
    E1 --> E2[准备掩码和掩码潜在表示]
    E2 --> E3[初始化噪声潜在表示]
    E3 --> E4[准备图像形状信息]
    
    E4 --> F[5. 时间步调度]
    F --> F1[计算噪声调度参数]
    F1 --> F2[生成时间步序列]
    F2 --> F3[计算动态偏移参数]
    F3 --> F4[根据强度调整时间步]
    
    F4 --> G[6. 去噪循环开始]
    G --> G1{遍历时间步}
    G1 --> G2[Transformer预测噪声]
    G2 --> G3{是否使用CFG?}
    G3 -->|是| G4[计算负面噪声预测]
    G3 -->|否| G5[应用调度器更新]
    G4 --> G6[应用CFG组合]
    G6 --> G5
    G5 --> G7[更新潜在表示]
    G7 --> G8[应用掩码混合]
    G8 --> G9[掩码区域与原图混合]
    G9 --> G10{是否有回调?}
    G10 -->|是| G11[执行回调函数]
    G10 -->|否| G12{是否完成所有步骤?}
    G11 --> G12
    G12 -->|否| G1
    G12 -->|是| H[7. 后处理和输出]
    
    H --> H1{输出类型检查}
    H1 -->|latent| H2[直接返回潜在表示]
    H1 -->|其他| H3[VAE解码潜在表示]
    H3 --> H4[图像后处理]
    H4 --> H5{是否需要应用覆盖?}
    H5 -->|是| H6[应用掩码覆盖]
    H5 -->|否| H7[格式转换]
    H6 --> H7
    H2 --> I[8. 返回结果]
    H7 --> I
    
    I --> I1{return_dict?}
    I1 -->|True| I2[返回QwenImagePipelineOutput]
    I1 -->|False| I3[返回元组]
    I2 --> J[结束]
    I3 --> J
    
    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style G fill:#fff3e0
    style H fill:#f3e5f5
    style C fill:#e8f5e8
    style G8 fill:#fff9c4
</div></code></pre>
<h2 id="%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82">技术细节</h2>
<h3 id="%E6%8E%A9%E7%A0%81%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6">掩码处理机制</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 掩码预处理</span>
mask_condition = self.mask_processor.preprocess(
    mask_image, 
    height=height, 
    width=width, 
    resize_mode=resize_mode, 
    crops_coords=crops_coords
)

<span class="hljs-comment"># 创建掩码图像</span>
<span class="hljs-keyword">if</span> masked_image_latents <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
    masked_image = init_image * (mask_condition &lt; <span class="hljs-number">0.5</span>)
<span class="hljs-keyword">else</span>:
    masked_image = masked_image_latents
</div></code></pre>
<h3 id="%E6%8E%A9%E7%A0%81%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5">掩码混合策略</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 在每个去噪步骤中应用掩码混合</span>
init_latents_proper = image_latents
init_mask = mask

<span class="hljs-keyword">if</span> i &lt; len(timesteps) - <span class="hljs-number">1</span>:
    noise_timestep = timesteps[i + <span class="hljs-number">1</span>]
    init_latents_proper = self.scheduler.scale_noise(
        init_latents_proper, torch.tensor([noise_timestep]), noise
    )

<span class="hljs-comment"># 关键：掩码区域使用去噪结果，非掩码区域保持原图</span>
latents = (<span class="hljs-number">1</span> - init_mask) * init_latents_proper + init_mask * latents
</div></code></pre>
<h3 id="%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF">提示模板</h3>
<p>QwenImageInpaintPipeline 使用专门的图像描述模板：</p>
<pre class="hljs"><code><div>template = <span class="hljs-string">"&lt;|im_start|&gt;system\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"</span>
</div></code></pre>
<h3 id="%E5%BC%BA%E5%BA%A6%E5%8F%82%E6%95%B0%E6%8E%A7%E5%88%B6">强度参数控制</h3>
<ul>
<li><strong>strength</strong>: 控制修复的强度</li>
<li>值越高，对原图的改变越大</li>
<li>值越低，更多保留原图内容</li>
<li>通过调整起始时间步实现</li>
</ul>
<h2 id="%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B">使用示例</h2>
<h3 id="%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95">基本用法</h3>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> QwenImageInpaintPipeline
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-comment"># 加载管道</span>
pipeline = QwenImageInpaintPipeline.from_pretrained(
    <span class="hljs-string">"Qwen/Qwen-Image"</span>, 
    torch_dtype=torch.bfloat16
)
pipeline.to(<span class="hljs-string">"cuda"</span>)

<span class="hljs-comment"># 加载图像和掩码</span>
original_image = Image.open(<span class="hljs-string">"original.jpg"</span>)
mask_image = Image.open(<span class="hljs-string">"mask.png"</span>)  <span class="hljs-comment"># 白色区域将被修复</span>

<span class="hljs-comment"># 执行修复</span>
result = pipeline(
    prompt=<span class="hljs-string">"黄色猫咪的脸，高分辨率，坐在公园长椅上"</span>,
    negative_prompt=<span class="hljs-string">"模糊，低质量"</span>,
    image=original_image,
    mask_image=mask_image,
    strength=<span class="hljs-number">0.85</span>,
    num_inference_steps=<span class="hljs-number">50</span>,
    true_cfg_scale=<span class="hljs-number">4.0</span>
)

<span class="hljs-comment"># 保存结果</span>
inpainted_image = result.images[<span class="hljs-number">0</span>]
inpainted_image.save(<span class="hljs-string">"inpainted_output.jpg"</span>)
</div></code></pre>
<h3 id="%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%E5%B8%A6%E8%A3%81%E5%89%AA">高级用法（带裁剪）</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 使用掩码裁剪优化性能</span>
result = pipeline(
    prompt=<span class="hljs-string">"现代艺术风格的花朵，色彩鲜艳"</span>,
    negative_prompt=<span class="hljs-string">"单调，无聊，模糊"</span>,
    image=original_image,
    mask_image=mask_image,
    padding_mask_crop=<span class="hljs-number">32</span>,  <span class="hljs-comment"># 裁剪填充</span>
    strength=<span class="hljs-number">0.7</span>,
    height=<span class="hljs-number">1024</span>,
    width=<span class="hljs-number">1024</span>,
    num_inference_steps=<span class="hljs-number">75</span>,
    true_cfg_scale=<span class="hljs-number">5.0</span>,
    generator=torch.manual_seed(<span class="hljs-number">42</span>)
)
</div></code></pre>
<h3 id="%E6%89%B9%E9%87%8F%E4%BF%AE%E5%A4%8D">批量修复</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 批量处理多个提示</span>
prompts = [
    <span class="hljs-string">"红色玫瑰花"</span>,
    <span class="hljs-string">"蓝色郁金香"</span>,
    <span class="hljs-string">"黄色向日葵"</span>
]

result = pipeline(
    prompt=prompts,
    negative_prompt=<span class="hljs-string">"枯萎，死亡，丑陋"</span>,
    image=original_image,
    mask_image=mask_image,
    strength=<span class="hljs-number">0.8</span>,
    num_images_per_prompt=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 每个提示生成2张图</span>
    num_inference_steps=<span class="hljs-number">50</span>
)

<span class="hljs-comment"># 保存所有结果</span>
<span class="hljs-keyword">for</span> i, img <span class="hljs-keyword">in</span> enumerate(result.images):
    img.save(<span class="hljs-string">f"inpaint_result_<span class="hljs-subst">{i}</span>.jpg"</span>)
</div></code></pre>
<h2 id="%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE">性能优化建议</h2>
<ol>
<li><strong>强度设置</strong>: 0.6-0.9 范围内效果较好，根据修复需求调整</li>
<li><strong>掩码质量</strong>: 清晰的掩码边界能获得更好的修复效果</li>
<li><strong>裁剪优化</strong>: 使用 <code>padding_mask_crop</code> 可以显著提升性能</li>
<li><strong>推理步数</strong>: 50-75 步通常足够，复杂修复可增加到 100 步</li>
<li><strong>内存管理</strong>: 大图像修复时使用 <code>enable_model_cpu_offload()</code></li>
</ol>
<h2 id="%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">常见问题和解决方案</h2>
<h3 id="1-%E4%BF%AE%E5%A4%8D%E8%BE%B9%E7%95%8C%E4%B8%8D%E8%87%AA%E7%84%B6">1. 修复边界不自然</h3>
<ul>
<li>降低 <code>strength</code> 值到 0.6-0.7</li>
<li>使用更柔和的掩码边界</li>
<li>增加推理步数</li>
</ul>
<h3 id="2-%E4%BF%AE%E5%A4%8D%E5%86%85%E5%AE%B9%E4%B8%8E%E5%91%A8%E5%9B%B4%E4%B8%8D%E5%8D%8F%E8%B0%83">2. 修复内容与周围不协调</h3>
<ul>
<li>优化提示词，描述周围环境</li>
<li>调整 <code>true_cfg_scale</code> 到 3.0-5.0</li>
<li>使用负面提示排除不协调元素</li>
</ul>
<h3 id="3-%E4%BF%AE%E5%A4%8D%E5%8C%BA%E5%9F%9F%E8%BF%87%E4%BA%8E%E6%A8%A1%E7%B3%8A">3. 修复区域过于模糊</h3>
<ul>
<li>增加推理步数到 75-100</li>
<li>提高 <code>true_cfg_scale</code> 到 6.0-8.0</li>
<li>检查掩码是否过大</li>
</ul>
<h2 id="%E4%B8%8E%E5%85%B6%E4%BB%96-pipeline-%E7%9A%84%E5%8C%BA%E5%88%AB">与其他 Pipeline 的区别</h2>
<table>
<thead>
<tr>
<th>特性</th>
<th>QwenImageInpaintPipeline</th>
<th>StableDiffusionInpaintPipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本编码器</td>
<td>Qwen2.5-VL (多模态)</td>
<td>CLIP (纯文本)</td>
</tr>
<tr>
<td>图像理解</td>
<td>原生支持</td>
<td>需要额外处理</td>
</tr>
<tr>
<td>掩码处理</td>
<td>专用处理器</td>
<td>标准处理</td>
</tr>
<tr>
<td>修复质量</td>
<td>高（理解图像语义）</td>
<td>中等</td>
</tr>
<tr>
<td>边界融合</td>
<td>智能融合</td>
<td>标准融合</td>
</tr>
<tr>
<td>提示模板</td>
<td>专用描述模板</td>
<td>通用模板</td>
</tr>
</tbody>
</table>

</body>
</html>
