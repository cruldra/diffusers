<!DOCTYPE html>
<html>
<head>
<title>DiffusionPipeline_Methods_Detailed.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="diffusionpipeline-%E7%B1%BB%E6%96%B9%E6%B3%95%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E">DiffusionPipeline 类方法详细说明</h1>
<h2 id="%E7%9B%AE%E5%BD%95">目录</h2>
<ul>
<li><a href="#%E7%B1%BB%E5%B1%9E%E6%80%A7">类属性</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95">核心方法</a></li>
<li><a href="#%E8%AE%BE%E5%A4%87%E5%92%8C%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86">设备和内存管理</a></li>
<li><a href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96">注意力优化</a></li>
<li><a href="#vae%E4%BC%98%E5%8C%96">VAE优化</a></li>
<li><a href="#%E5%B7%A5%E5%85%B7%E6%96%B9%E6%B3%95">工具方法</a></li>
<li><a href="#%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD">高级功能</a></li>
</ul>
<h2 id="%E7%B1%BB%E5%B1%9E%E6%80%A7">类属性</h2>
<h3 id="%E5%9F%BA%E7%A1%80%E5%B1%9E%E6%80%A7">基础属性</h3>
<table>
<thead>
<tr>
<th>属性名</th>
<th>类型</th>
<th>默认值</th>
<th>原始描述</th>
<th>中文说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>config_name</code></td>
<td><code>str</code></td>
<td><code>&quot;model_index.json&quot;</code></td>
<td>The configuration filename that stores the class and module names of all the diffusion pipeline's components.</td>
<td>存储扩散Pipeline所有组件的类名和模块名的配置文件名</td>
</tr>
<tr>
<td><code>model_cpu_offload_seq</code></td>
<td><code>Optional[str]</code></td>
<td><code>None</code></td>
<td>-</td>
<td>模型CPU卸载序列，定义模型卸载顺序</td>
</tr>
<tr>
<td><code>hf_device_map</code></td>
<td><code>Optional[Dict]</code></td>
<td><code>None</code></td>
<td>-</td>
<td>HuggingFace设备映射配置</td>
</tr>
<tr>
<td><code>_optional_components</code></td>
<td><code>List[str]</code></td>
<td><code>[]</code></td>
<td>List of all optional components that don't have to be passed to the pipeline to function (should be overridden by subclasses).</td>
<td>所有可选组件的列表，这些组件不必传递给Pipeline即可运行（应由子类重写）</td>
</tr>
<tr>
<td><code>_exclude_from_cpu_offload</code></td>
<td><code>List[str]</code></td>
<td><code>[]</code></td>
<td>-</td>
<td>从CPU卸载中排除的组件列表</td>
</tr>
<tr>
<td><code>_load_connected_pipes</code></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>-</td>
<td>是否加载连接的Pipeline</td>
</tr>
<tr>
<td><code>_is_onnx</code></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>-</td>
<td>是否为ONNX Pipeline</td>
</tr>
</tbody>
</table>
<h2 id="%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95">核心方法</h2>
<h3 id="1-init-%E6%96%B9%E6%B3%95">1. <code>__init__</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, *args, **kwargs)</span>
</span></div></code></pre>
<p><strong>描述</strong>: Pipeline的初始化方法，由子类实现具体的组件初始化。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>*args</code>: 位置参数，通常包含Pipeline的各个组件</li>
<li><code>**kwargs</code>: 关键字参数，用于传递额外配置</li>
</ul>
<hr>
<h3 id="2-registermodules-%E6%96%B9%E6%B3%95">2. <code>register_modules</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">register_modules</span><span class="hljs-params">(self, **kwargs)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 注册Pipeline的模块组件，将组件信息保存到配置中。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>**kwargs</code>: 要注册的模块，键为模块名，值为模块对象</li>
</ul>
<p><strong>功能</strong>:</p>
<ul>
<li>自动检测模块的库和类名</li>
<li>将模块信息注册到配置中</li>
<li>设置模块为Pipeline的属性</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div>pipeline.register_modules(
    unet=unet_model,
    vae=vae_model,
    scheduler=scheduler
)
</div></code></pre>
<hr>
<h3 id="3-savepretrained-%E6%96%B9%E6%B3%95">3. <code>save_pretrained</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_pretrained</span><span class="hljs-params">(
    self,
    save_directory: Union[str, os.PathLike],
    safe_serialization: bool = True,
    variant: Optional[str] = None,
    max_shard_size: Optional[Union[int, str]] = None,
    push_to_hub: bool = False,
    **kwargs
)</span>
</span></div></code></pre>
<p><strong>原始文档</strong>: Save all saveable variables of the pipeline to a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading method. The pipeline is easily reloaded using the [<code>~DiffusionPipeline.from_pretrained</code>] class method.</p>
<p><strong>中文描述</strong>: 将Pipeline的所有可保存变量保存到目录中。如果Pipeline变量的类实现了保存和加载方法，则可以保存和加载该变量。可以使用[<code>~DiffusionPipeline.from_pretrained</code>]类方法轻松重新加载Pipeline。</p>
<p><strong>参数详解</strong>:</p>
<ul>
<li><code>save_directory</code> (<code>str</code> or <code>os.PathLike</code>): Directory to save a pipeline to. Will be created if it doesn't exist. | 保存Pipeline的目录，如果不存在将被创建</li>
<li><code>safe_serialization</code> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>): Whether to save the model using <code>safetensors</code> or the traditional PyTorch way with <code>pickle</code>. | 是否使用<code>safetensors</code>保存模型，或使用传统的PyTorch方式与<code>pickle</code></li>
<li><code>variant</code> (<code>str</code>, <em>optional</em>): If specified, weights are saved in the format <code>pytorch_model.&lt;variant&gt;.bin</code>. | 如果指定，权重将以<code>pytorch_model.&lt;variant&gt;.bin</code>格式保存</li>
<li><code>max_shard_size</code> (<code>int</code> or <code>str</code>, defaults to <code>None</code>): The maximum size for a checkpoint before being sharded. | 分片前检查点的最大大小</li>
<li><code>push_to_hub</code> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>): Whether or not to push your model to the Hugging Face model hub after saving it. | 保存后是否将模型推送到Hugging Face模型中心</li>
<li><code>**kwargs</code> (<code>Dict[str, Any]</code>, <em>optional</em>): Additional keyword arguments passed along to the [<code>~utils.PushToHubMixin.push_to_hub</code>] method. | 传递给[<code>~utils.PushToHubMixin.push_to_hub</code>]方法的额外关键字参数</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 基础保存</span>
pipeline.save_pretrained(<span class="hljs-string">"./my_pipeline"</span>)

<span class="hljs-comment"># 保存fp16变体并推送到Hub</span>
pipeline.save_pretrained(
    <span class="hljs-string">"./my_pipeline"</span>,
    variant=<span class="hljs-string">"fp16"</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
    repo_id=<span class="hljs-string">"my_username/my_pipeline"</span>
)
</div></code></pre>
<hr>
<h3 id="4-frompretrained-%E7%B1%BB%E6%96%B9%E6%B3%95">4. <code>from_pretrained</code> 类方法</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@classmethod</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">from_pretrained</span><span class="hljs-params">(
    cls, 
    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], 
    **kwargs
)</span> -&gt; Self
</span></div></code></pre>
<p><strong>描述</strong>: 从预训练权重实例化PyTorch扩散Pipeline。</p>
<p><strong>核心参数</strong>:</p>
<ul>
<li><code>pretrained_model_name_or_path</code> (<code>str</code> | <code>os.PathLike</code>):
<ul>
<li>Hub仓库ID (如 <code>&quot;CompVis/ldm-text2im-large-256&quot;</code>)</li>
<li>本地目录路径 (如 <code>&quot;./my_pipeline_directory/&quot;</code>)</li>
<li>DDUF文件路径</li>
</ul>
</li>
</ul>
<p><strong>数据类型参数</strong>:</p>
<ul>
<li><code>torch_dtype</code> (<code>torch.dtype</code> | <code>dict</code>):
<ul>
<li>单一类型: <code>torch.float16</code></li>
<li>组件特定: <code>{'transformer': torch.bfloat16, 'vae': torch.float16}</code></li>
<li>带默认值: <code>{'transformer': torch.bfloat16, 'default': torch.float16}</code></li>
</ul>
</li>
</ul>
<p><strong>自定义Pipeline参数</strong>:</p>
<ul>
<li><code>custom_pipeline</code> (<code>str</code>):
<ul>
<li>Hub仓库ID: <code>&quot;hf-internal-testing/diffusers-dummy-pipeline&quot;</code></li>
<li>社区Pipeline名: <code>&quot;clip_guided_stable_diffusion&quot;</code></li>
<li>本地目录: <code>&quot;./my_pipeline_directory/&quot;</code></li>
</ul>
</li>
</ul>
<p><strong>下载控制参数</strong>:</p>
<ul>
<li><code>force_download</code> (<code>bool</code>, 默认 <code>False</code>): 强制重新下载</li>
<li><code>cache_dir</code> (<code>str</code>): 缓存目录路径</li>
<li><code>local_files_only</code> (<code>bool</code>, 默认 <code>False</code>): 仅使用本地文件</li>
<li><code>token</code> (<code>str</code> | <code>bool</code>): HuggingFace访问令牌</li>
<li><code>revision</code> (<code>str</code>, 默认 <code>&quot;main&quot;</code>): 模型版本分支/标签/提交ID</li>
<li><code>custom_revision</code> (<code>str</code>): 自定义Pipeline的版本</li>
</ul>
<p><strong>网络参数</strong>:</p>
<ul>
<li><code>proxies</code> (<code>Dict[str, str]</code>): 代理服务器配置</li>
<li><code>mirror</code> (<code>str</code>): 镜像源地址（中国用户）</li>
</ul>
<p><strong>设备映射参数</strong>:</p>
<ul>
<li><code>device_map</code> (<code>str</code>): 设备映射策略，目前支持&quot;balanced&quot;</li>
<li><code>max_memory</code> (<code>Dict</code>): 每个设备的最大内存限制</li>
<li><code>offload_folder</code> (<code>str</code>): 磁盘卸载目录</li>
<li><code>offload_state_dict</code> (<code>bool</code>): 是否临时卸载CPU状态字典</li>
</ul>
<p><strong>内存优化参数</strong>:</p>
<ul>
<li><code>low_cpu_mem_usage</code> (<code>bool</code>): 低CPU内存使用模式</li>
<li><code>use_safetensors</code> (<code>bool</code>): 是否使用safetensors格式</li>
<li><code>use_onnx</code> (<code>bool</code>): 是否使用ONNX权重</li>
<li><code>variant</code> (<code>str</code>): 权重变体，如&quot;fp16&quot;</li>
<li><code>dduf_file</code> (<code>str</code>): DDUF文件路径</li>
</ul>
<p><strong>其他参数</strong>:</p>
<ul>
<li><code>output_loading_info</code> (<code>bool</code>, 默认 <code>False</code>): 返回加载信息</li>
<li><code>quantization_config</code>: 量化配置</li>
<li><code>**kwargs</code>: 覆盖Pipeline组件的参数</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 基础加载</span>
pipeline = DiffusionPipeline.from_pretrained(<span class="hljs-string">"stable-diffusion-v1-5/stable-diffusion-v1-5"</span>)

<span class="hljs-comment"># 指定数据类型和设备</span>
pipeline = DiffusionPipeline.from_pretrained(
    <span class="hljs-string">"stabilityai/stable-diffusion-xl-base-1.0"</span>,
    torch_dtype=torch.float16,
    device_map=<span class="hljs-string">"balanced"</span>,
    use_safetensors=<span class="hljs-literal">True</span>
)

<span class="hljs-comment"># 组件特定数据类型</span>
pipeline = DiffusionPipeline.from_pretrained(
    <span class="hljs-string">"black-forest-labs/FLUX.1-schnell"</span>,
    torch_dtype={
        <span class="hljs-string">'transformer'</span>: torch.bfloat16,
        <span class="hljs-string">'vae'</span>: torch.float16,
        <span class="hljs-string">'default'</span>: torch.float32
    }
)

<span class="hljs-comment"># 使用自定义调度器</span>
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> LMSDiscreteScheduler
pipeline = DiffusionPipeline.from_pretrained(
    <span class="hljs-string">"stable-diffusion-v1-5/stable-diffusion-v1-5"</span>,
    scheduler=LMSDiscreteScheduler.from_config(scheduler_config)
)
</div></code></pre>
<hr>
<h3 id="5-to-%E6%96%B9%E6%B3%95">5. <code>to</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to</span><span class="hljs-params">(self, *args, **kwargs)</span> -&gt; Self
</span></div></code></pre>
<p><strong>描述</strong>: 执行Pipeline的数据类型和/或设备转换。</p>
<p><strong>调用方式</strong>:</p>
<ul>
<li><code>to(dtype)</code>: 转换到指定数据类型</li>
<li><code>to(device)</code>: 转换到指定设备</li>
<li><code>to(device, dtype)</code>: 同时转换设备和数据类型</li>
<li><code>to(device=None, dtype=None, silence_dtype_warnings=False)</code>: 关键字参数方式</li>
</ul>
<p><strong>参数</strong>:</p>
<ul>
<li><code>dtype</code> (<code>torch.dtype</code>, 可选): 目标数据类型</li>
<li><code>device</code> (<code>torch.device</code>, 可选): 目标设备</li>
<li><code>silence_dtype_warnings</code> (<code>bool</code>, 默认 <code>False</code>): 是否静默数据类型警告</li>
</ul>
<p><strong>返回值</strong>: 转换后的Pipeline实例（如果已经是目标类型则返回自身）</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 移动到GPU</span>
pipeline = pipeline.to(<span class="hljs-string">"cuda"</span>)

<span class="hljs-comment"># 转换数据类型</span>
pipeline = pipeline.to(torch.float16)

<span class="hljs-comment"># 同时转换设备和数据类型</span>
pipeline = pipeline.to(<span class="hljs-string">"cuda"</span>, torch.float16)

<span class="hljs-comment"># 静默警告</span>
pipeline = pipeline.to(<span class="hljs-string">"cuda"</span>, silence_dtype_warnings=<span class="hljs-literal">True</span>)
</div></code></pre>
<hr>
<h3 id="6-download-%E7%B1%BB%E6%96%B9%E6%B3%95">6. <code>download</code> 类方法</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@classmethod</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">download</span><span class="hljs-params">(cls, pretrained_model_name, **kwargs)</span> -&gt; Union[str, os.PathLike]
</span></div></code></pre>
<p><strong>描述</strong>: 下载并缓存PyTorch扩散Pipeline，但不实例化。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>pretrained_model_name</code> (<code>str</code>): Hub仓库ID</li>
<li><code>custom_pipeline</code> (<code>str</code>, 可选): 自定义Pipeline</li>
<li><code>force_download</code> (<code>bool</code>, 默认 <code>False</code>): 强制重新下载</li>
<li><code>proxies</code> (<code>Dict[str, str]</code>, 可选): 代理配置</li>
<li><code>output_loading_info</code> (<code>bool</code>, 默认 <code>False</code>): 返回加载信息</li>
<li><code>local_files_only</code> (<code>bool</code>, 默认 <code>False</code>): 仅使用本地文件</li>
<li><code>token</code> (<code>str</code> | <code>bool</code>, 可选): 访问令牌</li>
<li><code>revision</code> (<code>str</code>, 默认 <code>&quot;main&quot;</code>): 版本</li>
</ul>
<p><strong>返回值</strong>: 下载的模型路径</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 下载模型到缓存</span>
model_path = DiffusionPipeline.download(<span class="hljs-string">"stable-diffusion-v1-5/stable-diffusion-v1-5"</span>)
print(<span class="hljs-string">f"模型下载到: <span class="hljs-subst">{model_path}</span>"</span>)
</div></code></pre>
<h2 id="%E5%B1%9E%E6%80%A7%E8%AE%BF%E9%97%AE%E5%99%A8">属性访问器</h2>
<h3 id="7-device-%E5%B1%9E%E6%80%A7">7. <code>device</code> 属性</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@property</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">device</span><span class="hljs-params">(self)</span> -&gt; torch.device
</span></div></code></pre>
<p><strong>描述</strong>: 返回Pipeline所在的torch设备。</p>
<p><strong>返回值</strong>: Pipeline当前所在的设备</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div>print(<span class="hljs-string">f"Pipeline在设备: <span class="hljs-subst">{pipeline.device}</span>"</span>)
<span class="hljs-comment"># 输出: Pipeline在设备: cuda:0</span>
</div></code></pre>
<hr>
<h3 id="8-dtype-%E5%B1%9E%E6%80%A7">8. <code>dtype</code> 属性</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@property</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dtype</span><span class="hljs-params">(self)</span> -&gt; torch.dtype
</span></div></code></pre>
<p><strong>描述</strong>: 返回Pipeline的torch数据类型。</p>
<p><strong>返回值</strong>: Pipeline的数据类型</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div>print(<span class="hljs-string">f"Pipeline数据类型: <span class="hljs-subst">{pipeline.dtype}</span>"</span>)
<span class="hljs-comment"># 输出: Pipeline数据类型: torch.float16</span>
</div></code></pre>
<hr>
<h3 id="9-components-%E5%B1%9E%E6%80%A7">9. <code>components</code> 属性</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@property</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">components</span><span class="hljs-params">(self)</span> -&gt; Dict[str, Any]
</span></div></code></pre>
<p><strong>描述</strong>: 返回Pipeline的所有组件字典，用于在不同Pipeline间共享权重和配置。</p>
<p><strong>返回值</strong>: 包含所有Pipeline组件的字典</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div>components = pipeline.components
print(<span class="hljs-string">"Pipeline组件:"</span>, list(components.keys()))
<span class="hljs-comment"># 输出: Pipeline组件: ['vae', 'text_encoder', 'tokenizer', 'unet', 'scheduler']</span>

<span class="hljs-comment"># 使用组件创建新Pipeline</span>
new_pipeline = AnotherPipeline(**components)
</div></code></pre>
<hr>
<h3 id="10-nameorpath-%E5%B1%9E%E6%80%A7">10. <code>name_or_path</code> 属性</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@property</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">name_or_path</span><span class="hljs-params">(self)</span> -&gt; str
</span></div></code></pre>
<p><strong>描述</strong>: 返回Pipeline的名称或路径。</p>
<p><strong>返回值</strong>: Pipeline的原始名称或路径</p>
<h2 id="%E8%AE%BE%E5%A4%87%E5%92%8C%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86">设备和内存管理</h2>
<h3 id="11-enablemodelcpuoffload-%E6%96%B9%E6%B3%95">11. <code>enable_model_cpu_offload</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_model_cpu_offload</span><span class="hljs-params">(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = None)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 使用accelerate将所有模型卸载到CPU，以较低的性能影响减少内存使用。与<code>enable_sequential_cpu_offload</code>相比，此方法在调用<code>forward</code>方法时将整个模型移动到加速器，模型保持在加速器上直到下一个模型运行。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>gpu_id</code> (<code>int</code>, 可选): 推理时使用的加速器ID，默认为0</li>
<li><code>device</code> (<code>torch.device</code> | <code>str</code>, 可选): 推理时使用的PyTorch设备类型，自动检测可用加速器</li>
</ul>
<p><strong>特点</strong>:</p>
<ul>
<li>内存节省低于<code>enable_sequential_cpu_offload</code></li>
<li>性能影响较小，因为UNet的迭代执行</li>
<li>适合需要平衡内存和性能的场景</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 使用默认GPU</span>
pipeline.enable_model_cpu_offload()

<span class="hljs-comment"># 指定GPU ID</span>
pipeline.enable_model_cpu_offload(gpu_id=<span class="hljs-number">1</span>)

<span class="hljs-comment"># 指定设备</span>
pipeline.enable_model_cpu_offload(device=<span class="hljs-string">"cuda:1"</span>)
</div></code></pre>
<hr>
<h3 id="12-enablesequentialcpuoffload-%E6%96%B9%E6%B3%95">12. <code>enable_sequential_cpu_offload</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_sequential_cpu_offload</span><span class="hljs-params">(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = None)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 使用🤗 Accelerate将所有模型卸载到CPU，显著减少内存使用。所有<code>torch.nn.Module</code>组件的状态字典保存到CPU，然后移动到<code>torch.device('meta')</code>，仅在特定子模块调用<code>forward</code>方法时加载到加速器。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>gpu_id</code> (<code>int</code>, 可选): 推理时使用的加速器ID，默认为0</li>
<li><code>device</code> (<code>torch.device</code> | <code>str</code>, 可选): 推理时使用的PyTorch设备类型</li>
</ul>
<p><strong>特点</strong>:</p>
<ul>
<li>内存节省高于<code>enable_model_cpu_offload</code></li>
<li>性能影响较大，因为子模块级别的卸载</li>
<li>适合内存极度受限的场景</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 启用顺序CPU卸载</span>
pipeline.enable_sequential_cpu_offload()

<span class="hljs-comment"># 指定设备</span>
pipeline.enable_sequential_cpu_offload(device=<span class="hljs-string">"cuda:0"</span>)
</div></code></pre>
<hr>
<h3 id="13-removeallhooks-%E6%96%B9%E6%B3%95">13. <code>remove_all_hooks</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">remove_all_hooks</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 移除使用<code>enable_sequential_cpu_offload</code>或<code>enable_model_cpu_offload</code>时添加的所有钩子。</p>
<p><strong>功能</strong>:</p>
<ul>
<li>清理所有accelerate钩子</li>
<li>恢复模型到正常状态</li>
<li>释放钩子占用的资源</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 移除所有卸载钩子</span>
pipeline.remove_all_hooks()
</div></code></pre>
<hr>
<h3 id="14-maybefreemodelhooks-%E6%96%B9%E6%B3%95">14. <code>maybe_free_model_hooks</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maybe_free_model_hooks</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 执行以下操作的方法：</p>
<ul>
<li>卸载所有组件</li>
<li>移除使用<code>enable_model_cpu_offload</code>时添加的所有模型钩子，然后重新应用</li>
<li>重置去噪器组件的有状态扩散钩子</li>
</ul>
<p><strong>用途</strong>: 在Pipeline的<code>__call__</code>函数末尾添加此函数，确保在应用<code>enable_model_cpu_offload</code>时正常工作。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 通常在Pipeline内部调用</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self, *args, **kwargs)</span>:</span>
    <span class="hljs-comment"># ... Pipeline逻辑 ...</span>
    self.maybe_free_model_hooks()
    <span class="hljs-keyword">return</span> result
</div></code></pre>
<hr>
<h3 id="15-resetdevicemap-%E6%96%B9%E6%B3%95">15. <code>reset_device_map</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_device_map</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 将设备映射（如果有）重置为None。</p>
<p><strong>功能</strong>:</p>
<ul>
<li>移除所有钩子</li>
<li>将所有组件移动到CPU</li>
<li>清空设备映射配置</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 重置设备映射</span>
pipeline.reset_device_map()
</div></code></pre>
<h2 id="%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BC%98%E5%8C%96">注意力优化</h2>
<h3 id="16-enablexformersmemoryefficientattention-%E6%96%B9%E6%B3%95">16. <code>enable_xformers_memory_efficient_attention</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_xformers_memory_efficient_attention</span><span class="hljs-params">(self, attention_op: Optional[Callable] = None)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用来自xFormers的内存高效注意力。启用此选项时，应该观察到更低的GPU内存使用和推理期间的潜在加速。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>attention_op</code> (<code>Callable</code>, 可选): 覆盖默认的<code>None</code>操作符，用作xFormers的<code>memory_efficient_attention()</code>函数的<code>op</code>参数</li>
</ul>
<p><strong>注意事项</strong>:</p>
<ul>
<li>⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先</li>
<li>训练期间的加速不保证</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 启用xFormers内存高效注意力</span>
pipeline.enable_xformers_memory_efficient_attention()

<span class="hljs-comment"># 使用特定操作符</span>
<span class="hljs-keyword">from</span> xformers.ops <span class="hljs-keyword">import</span> MemoryEfficientAttentionFlashAttentionOp
pipeline.enable_xformers_memory_efficient_attention(
    attention_op=MemoryEfficientAttentionFlashAttentionOp
)

<span class="hljs-comment"># VAE的Flash Attention变通方案</span>
pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=<span class="hljs-literal">None</span>)
</div></code></pre>
<hr>
<h3 id="17-disablexformersmemoryefficientattention-%E6%96%B9%E6%B3%95">17. <code>disable_xformers_memory_efficient_attention</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">disable_xformers_memory_efficient_attention</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 禁用来自xFormers的内存高效注意力。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用xFormers内存高效注意力</span>
pipeline.disable_xformers_memory_efficient_attention()
</div></code></pre>
<hr>
<h3 id="18-enableattentionslicing-%E6%96%B9%E6%B3%95">18. <code>enable_attention_slicing</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_attention_slicing</span><span class="hljs-params">(self, slice_size: Optional[Union[str, int]] = <span class="hljs-string">"auto"</span>)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成切片，分几个步骤计算注意力。对于多个注意力头，计算在每个头上顺序执行。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>slice_size</code> (<code>str</code> | <code>int</code>, 默认 <code>&quot;auto&quot;</code>):
<ul>
<li><code>&quot;auto&quot;</code>: 将注意力头的输入减半，注意力将分两步计算</li>
<li><code>&quot;max&quot;</code>: 通过一次只运行一个切片来节省最大内存</li>
<li>数字: 使用<code>attention_head_dim // slice_size</code>个切片，<code>attention_head_dim</code>必须是<code>slice_size</code>的倍数</li>
</ul>
</li>
</ul>
<p><strong>注意事项</strong>:</p>
<ul>
<li>⚠️ 如果已经使用PyTorch 2.0的<code>scaled_dot_product_attention</code>(SDPA)或xFormers，不要启用注意力切片</li>
<li>这些注意力计算已经非常内存高效，启用切片可能导致严重减速</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 自动切片</span>
pipeline.enable_attention_slicing()

<span class="hljs-comment"># 最大内存节省</span>
pipeline.enable_attention_slicing(<span class="hljs-string">"max"</span>)

<span class="hljs-comment"># 自定义切片大小</span>
pipeline.enable_attention_slicing(<span class="hljs-number">4</span>)
</div></code></pre>
<hr>
<h3 id="19-disableattentionslicing-%E6%96%B9%E6%B3%95">19. <code>disable_attention_slicing</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">disable_attention_slicing</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 禁用切片注意力计算。如果之前调用了<code>enable_attention_slicing</code>，注意力将在一步中计算。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用注意力切片</span>
pipeline.disable_attention_slicing()
</div></code></pre>
<hr>
<h3 id="20-setattentionslice-%E6%96%B9%E6%B3%95">20. <code>set_attention_slice</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_attention_slice</span><span class="hljs-params">(self, slice_size: Optional[int])</span>
</span></div></code></pre>
<p><strong>描述</strong>: 设置注意力切片大小的内部方法。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>slice_size</code> (<code>int</code>, 可选): 切片大小，<code>None</code>表示禁用切片</li>
</ul>
<p><strong>功能</strong>: 遍历所有支持注意力切片的模块并设置切片大小</p>
<h2 id="vae%E4%BC%98%E5%8C%96">VAE优化</h2>
<h3 id="21-enablevaeslicing-%E6%96%B9%E6%B3%95">21. <code>enable_vae_slicing</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_vae_slicing</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用切片VAE解码。启用此选项时，VAE将输入张量分割成切片，分几个步骤计算解码。这对节省内存和允许更大的批次大小很有用。</p>
<p><strong>特点</strong>:</p>
<ul>
<li>内存节省：中等</li>
<li>性能影响：很小</li>
<li>适用场景：需要处理大批次或高分辨率图像</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 启用VAE切片</span>
pipeline.enable_vae_slicing()
</div></code></pre>
<hr>
<h3 id="22-disablevaeslicing-%E6%96%B9%E6%B3%95">22. <code>disable_vae_slicing</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">disable_vae_slicing</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 禁用切片VAE解码。如果之前启用了<code>enable_vae_slicing</code>，此方法将回到一步计算解码。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用VAE切片</span>
pipeline.disable_vae_slicing()
</div></code></pre>
<hr>
<h3 id="23-enablevaetiling-%E6%96%B9%E6%B3%95">23. <code>enable_vae_tiling</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_vae_tiling</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用平铺VAE解码。启用此选项时，VAE将输入张量分割成瓦片，分几个步骤计算解码和编码。这对节省大量内存和允许处理更大图像非常有用。</p>
<p><strong>特点</strong>:</p>
<ul>
<li>内存节省：高</li>
<li>性能影响：轻微</li>
<li>适用场景：处理超高分辨率图像或内存严重受限</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 启用VAE平铺</span>
pipeline.enable_vae_tiling()
</div></code></pre>
<hr>
<h3 id="24-disablevaetiling-%E6%96%B9%E6%B3%95">24. <code>disable_vae_tiling</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">disable_vae_tiling</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 禁用平铺VAE解码。如果之前启用了<code>enable_vae_tiling</code>，此方法将回到一步计算解码。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用VAE平铺</span>
pipeline.disable_vae_tiling()
</div></code></pre>
<h2 id="%E5%B7%A5%E5%85%B7%E6%96%B9%E6%B3%95">工具方法</h2>
<h3 id="25-progressbar-%E6%96%B9%E6%B3%95">25. <code>progress_bar</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">progress_bar</span><span class="hljs-params">(self, iterable=None, total=None)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 创建进度条用于显示推理进度。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>iterable</code> (可选): 可迭代对象，用于包装进度条</li>
<li><code>total</code> (可选): 总步数，用于创建手动更新的进度条</li>
</ul>
<p><strong>返回值</strong>: tqdm进度条对象</p>
<p><strong>注意</strong>: <code>iterable</code>和<code>total</code>必须提供其中一个</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 包装可迭代对象</span>
<span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> pipeline.progress_bar(range(<span class="hljs-number">50</span>)):
    <span class="hljs-comment"># 执行推理步骤</span>
    <span class="hljs-keyword">pass</span>

<span class="hljs-comment"># 手动进度条</span>
pbar = pipeline.progress_bar(total=<span class="hljs-number">50</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):
    <span class="hljs-comment"># 执行操作</span>
    pbar.update(<span class="hljs-number">1</span>)
</div></code></pre>
<hr>
<h3 id="26-setprogressbarconfig-%E6%96%B9%E6%B3%95">26. <code>set_progress_bar_config</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_progress_bar_config</span><span class="hljs-params">(self, **kwargs)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 设置进度条配置参数。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>**kwargs</code>: tqdm进度条的配置参数</li>
</ul>
<p><strong>常用配置</strong>:</p>
<ul>
<li><code>disable</code> (<code>bool</code>): 是否禁用进度条</li>
<li><code>desc</code> (<code>str</code>): 进度条描述</li>
<li><code>leave</code> (<code>bool</code>): 完成后是否保留进度条</li>
<li><code>position</code> (<code>int</code>): 进度条位置</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用进度条</span>
pipeline.set_progress_bar_config(disable=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># 自定义进度条</span>
pipeline.set_progress_bar_config(
    desc=<span class="hljs-string">"生成图像"</span>,
    leave=<span class="hljs-literal">False</span>,
    position=<span class="hljs-number">0</span>
)
</div></code></pre>
<hr>
<h3 id="27-numpytopil-%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95">27. <code>numpy_to_pil</code> 静态方法</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@staticmethod</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">numpy_to_pil</span><span class="hljs-params">(images)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 将NumPy图像或图像批次转换为PIL图像。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>images</code>: NumPy数组格式的图像</li>
</ul>
<p><strong>返回值</strong>: PIL图像列表</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-comment"># 转换NumPy图像为PIL</span>
numpy_images = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>)
pil_images = DiffusionPipeline.numpy_to_pil(numpy_images)
</div></code></pre>
<hr>
<h3 id="28-frompipe-%E7%B1%BB%E6%96%B9%E6%B3%95">28. <code>from_pipe</code> 类方法</h3>
<pre class="hljs"><code><div><span class="hljs-meta">@classmethod</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">from_pipe</span><span class="hljs-params">(cls, pipeline, **kwargs)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 从给定Pipeline创建新Pipeline。此方法对于从现有Pipeline组件创建新Pipeline而不重新分配额外内存很有用。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>pipeline</code> (<code>DiffusionPipeline</code>): 源Pipeline</li>
<li><code>**kwargs</code>: 覆盖组件或配置的参数</li>
</ul>
<p><strong>返回值</strong>: 具有相同权重和配置的新Pipeline</p>
<p><strong>特点</strong>:</p>
<ul>
<li>重用现有组件，节省内存</li>
<li>可以覆盖特定组件</li>
<li>保持原始配置</li>
</ul>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline, StableDiffusionSAGPipeline

<span class="hljs-comment"># 创建基础Pipeline</span>
base_pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">"stable-diffusion-v1-5/stable-diffusion-v1-5"</span>)

<span class="hljs-comment"># 从基础Pipeline创建SAG Pipeline</span>
sag_pipe = StableDiffusionSAGPipeline.from_pipe(base_pipe)

<span class="hljs-comment"># 覆盖特定组件</span>
new_pipe = StableDiffusionPipeline.from_pipe(
    base_pipe,
    scheduler=new_scheduler,
    torch_dtype=torch.float16
)
</div></code></pre>
<h2 id="%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD">高级功能</h2>
<h3 id="29-enablefreeu-%E6%96%B9%E6%B3%95">29. <code>enable_freeu</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_freeu</span><span class="hljs-params">(self, s1: float, s2: float, b1: float, b2: float)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用FreeU机制，如论文https://huggingface.co/papers/2309.11497所述。缩放因子后的后缀表示应用的阶段。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>s1</code> (<code>float</code>): 阶段1的缩放因子，用于减弱跳跃特征的贡献，缓解增强去噪过程中的&quot;过度平滑效应&quot;</li>
<li><code>s2</code> (<code>float</code>): 阶段2的缩放因子，用于减弱跳跃特征的贡献</li>
<li><code>b1</code> (<code>float</code>): 阶段1的缩放因子，用于放大骨干特征的贡献</li>
<li><code>b2</code> (<code>float</code>): 阶段2的缩放因子，用于放大骨干特征的贡献</li>
</ul>
<p><strong>适用模型</strong>: 需要Pipeline具有<code>unet</code>组件</p>
<p><strong>推荐值</strong>: 请参考官方仓库获取适用于不同Pipeline的已知有效值组合</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Stable Diffusion v1.5推荐值</span>
pipeline.enable_freeu(s1=<span class="hljs-number">0.9</span>, s2=<span class="hljs-number">0.2</span>, b1=<span class="hljs-number">1.2</span>, b2=<span class="hljs-number">1.4</span>)

<span class="hljs-comment"># Stable Diffusion XL推荐值</span>
pipeline.enable_freeu(s1=<span class="hljs-number">0.6</span>, s2=<span class="hljs-number">0.4</span>, b1=<span class="hljs-number">1.1</span>, b2=<span class="hljs-number">1.2</span>)
</div></code></pre>
<hr>
<h3 id="30-disablefreeu-%E6%96%B9%E6%B3%95">30. <code>disable_freeu</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">disable_freeu</span><span class="hljs-params">(self)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 如果启用了FreeU机制，则禁用它。</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 禁用FreeU</span>
pipeline.disable_freeu()
</div></code></pre>
<hr>
<h3 id="31-fuseqkvprojections-%E6%96%B9%E6%B3%95">31. <code>fuse_qkv_projections</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fuse_qkv_projections</span><span class="hljs-params">(self, unet: bool = True, vae: bool = True)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 启用融合QKV投影。对于自注意力模块，所有投影矩阵（查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>unet</code> (<code>bool</code>, 默认 <code>True</code>): 是否在UNet上应用融合</li>
<li><code>vae</code> (<code>bool</code>, 默认 <code>True</code>): 是否在VAE上应用融合</li>
</ul>
<p><strong>注意</strong>: 🧪 这是实验性API</p>
<p><strong>限制</strong>: VAE融合仅支持<code>AutoencoderKL</code>类型</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 融合UNet和VAE的QKV投影</span>
pipeline.fuse_qkv_projections()

<span class="hljs-comment"># 仅融合UNet</span>
pipeline.fuse_qkv_projections(unet=<span class="hljs-literal">True</span>, vae=<span class="hljs-literal">False</span>)
</div></code></pre>
<hr>
<h3 id="32-unfuseqkvprojections-%E6%96%B9%E6%B3%95">32. <code>unfuse_qkv_projections</code> 方法</h3>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">unfuse_qkv_projections</span><span class="hljs-params">(self, unet: bool = True, vae: bool = True)</span>
</span></div></code></pre>
<p><strong>描述</strong>: 如果启用了QKV投影融合，则禁用它。</p>
<p><strong>参数</strong>:</p>
<ul>
<li><code>unet</code> (<code>bool</code>, 默认 <code>True</code>): 是否在UNet上取消融合</li>
<li><code>vae</code> (<code>bool</code>, 默认 <code>True</code>): 是否在VAE上取消融合</li>
</ul>
<p><strong>注意</strong>: 🧪 这是实验性API</p>
<p><strong>示例</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 取消融合QKV投影</span>
pipeline.unfuse_qkv_projections()
</div></code></pre>
<h2 id="%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E5%AF%B9%E6%AF%94">内存优化策略对比</h2>
<table>
<thead>
<tr>
<th>方法</th>
<th>内存节省</th>
<th>性能影响</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>enable_model_cpu_offload()</code></td>
<td>中等</td>
<td>轻微</td>
<td>平衡内存和性能</td>
</tr>
<tr>
<td><code>enable_sequential_cpu_offload()</code></td>
<td>高</td>
<td>中等</td>
<td>内存极度受限</td>
</tr>
<tr>
<td><code>enable_attention_slicing()</code></td>
<td>中等</td>
<td>轻微</td>
<td>大批次推理</td>
</tr>
<tr>
<td><code>enable_vae_slicing()</code></td>
<td>低</td>
<td>很轻微</td>
<td>VAE内存优化</td>
</tr>
<tr>
<td><code>enable_vae_tiling()</code></td>
<td>高</td>
<td>轻微</td>
<td>超高分辨率图像</td>
</tr>
<tr>
<td><code>enable_xformers_memory_efficient_attention()</code></td>
<td>中等</td>
<td>负影响(加速)</td>
<td>有xFormers环境</td>
</tr>
</tbody>
</table>
<h2 id="%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">最佳实践</h2>
<h3 id="%E5%86%85%E5%AD%98%E5%8F%97%E9%99%90%E7%8E%AF%E5%A2%83">内存受限环境</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 最大内存节省配置</span>
pipeline.enable_sequential_cpu_offload()
pipeline.enable_attention_slicing(<span class="hljs-string">"max"</span>)
pipeline.enable_vae_slicing()
pipeline.enable_vae_tiling()
</div></code></pre>
<h3 id="%E6%80%A7%E8%83%BD%E4%BC%98%E5%85%88%E7%8E%AF%E5%A2%83">性能优先环境</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 性能优化配置</span>
pipeline.to(<span class="hljs-string">"cuda"</span>, torch.float16)
pipeline.enable_xformers_memory_efficient_attention()
pipeline.enable_model_cpu_offload()  <span class="hljs-comment"># 轻微内存节省</span>
</div></code></pre>
<h3 id="%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90">高分辨率图像生成</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 高分辨率优化</span>
pipeline.enable_vae_tiling()
pipeline.enable_attention_slicing(<span class="hljs-string">"auto"</span>)
pipeline.enable_model_cpu_offload()
</div></code></pre>

</body>
</html>
