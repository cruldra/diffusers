<!DOCTYPE html>
<html>
<head>
<title>QwenImageImg2ImgPipeline__call__方法详解.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="qwenimageimg2imgpipeline-call-%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3">QwenImageImg2ImgPipeline <code>__call__</code> 方法详解</h1>
<h2 id="%E6%A6%82%E8%BF%B0">概述</h2>
<p><code>QwenImageImg2ImgPipeline</code> 是基于 Qwen 多模态大模型的图像到图像转换管道，其 <code>__call__</code> 方法是执行图像风格转换和内容变换的核心接口。该方法接受原始图像和文本提示，通过扩散模型生成基于原图结构的新图像。</p>
<h2 id="%E6%96%B9%E6%B3%95%E7%AD%BE%E5%90%8D">方法签名</h2>
<pre class="hljs"><code><div><span class="hljs-meta">@torch.no_grad()</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(
    self,
    prompt: Union[str, List[str]] = None,
    negative_prompt: Union[str, List[str]] = None,
    true_cfg_scale: float = <span class="hljs-number">4.0</span>,
    image: PipelineImageInput = None,
    height: Optional[int] = None,
    width: Optional[int] = None,
    strength: float = <span class="hljs-number">0.6</span>,
    num_inference_steps: int = <span class="hljs-number">50</span>,
    sigmas: Optional[List[float]] = None,
    guidance_scale: float = <span class="hljs-number">1.0</span>,
    num_images_per_prompt: int = <span class="hljs-number">1</span>,
    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
    latents: Optional[torch.Tensor] = None,
    prompt_embeds: Optional[torch.Tensor] = None,
    prompt_embeds_mask: Optional[torch.Tensor] = None,
    negative_prompt_embeds: Optional[torch.Tensor] = None,
    negative_prompt_embeds_mask: Optional[torch.Tensor] = None,
    output_type: Optional[str] = <span class="hljs-string">"pil"</span>,
    return_dict: bool = True,
    attention_kwargs: Optional[Dict[str, Any]] = None,
    callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
    callback_on_step_end_tensor_inputs: List[str] = [<span class="hljs-string">"latents"</span>],
    max_sequence_length: int = <span class="hljs-number">512</span>,
)</span> -&gt; Union[QwenImagePipelineOutput, Tuple]:</span>
</div></code></pre>
<h2 id="%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3">参数详解</h2>
<h3 id="%E5%BF%85%E9%9C%80%E5%8F%82%E6%95%B0">必需参数</h3>
<ul>
<li><strong><code>prompt</code></strong> (<code>str</code> 或 <code>List[str]</code>): 文本提示，描述目标图像的风格或内容</li>
<li><strong><code>image</code></strong> (<code>PipelineImageInput</code>): 原始输入图像，作为转换的基础</li>
</ul>
<h3 id="img2img-%E7%89%B9%E6%9C%89%E5%8F%82%E6%95%B0">Img2Img 特有参数</h3>
<ul>
<li><strong><code>strength</code></strong> (<code>float</code>, 默认 0.6): 转换强度，控制对原图的保留程度
<ul>
<li>0.0: 完全保留原图</li>
<li>1.0: 完全重新生成</li>
</ul>
</li>
</ul>
<h3 id="%E6%8E%A7%E5%88%B6%E5%8F%82%E6%95%B0">控制参数</h3>
<ul>
<li><strong><code>negative_prompt</code></strong> (<code>str</code> 或 <code>List[str]</code>, 可选): 负面提示</li>
<li><strong><code>true_cfg_scale</code></strong> (<code>float</code>, 默认 4.0): 真实 CFG 缩放因子</li>
<li><strong><code>guidance_scale</code></strong> (<code>float</code>, 默认 1.0): 引导缩放因子</li>
<li><strong><code>num_inference_steps</code></strong> (<code>int</code>, 默认 50): 推理步数</li>
</ul>
<h3 id="%E5%B0%BA%E5%AF%B8%E5%8F%82%E6%95%B0">尺寸参数</h3>
<ul>
<li><strong><code>height</code></strong> (<code>int</code>, 可选): 输出图像高度</li>
<li><strong><code>width</code></strong> (<code>int</code>, 可选): 输出图像宽度</li>
</ul>
<h3 id="%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6">生成控制</h3>
<ul>
<li><strong><code>num_images_per_prompt</code></strong> (<code>int</code>, 默认 1): 每个提示生成的图像数量</li>
<li><strong><code>generator</code></strong> (<code>torch.Generator</code>, 可选): 随机数生成器</li>
<li><strong><code>sigmas</code></strong> (<code>List[float]</code>, 可选): 自定义噪声调度参数</li>
</ul>
<h3 id="%E9%AB%98%E7%BA%A7%E5%8F%82%E6%95%B0">高级参数</h3>
<ul>
<li><strong><code>latents</code></strong> (<code>torch.Tensor</code>, 可选): 预计算的潜在表示</li>
<li><strong><code>prompt_embeds</code></strong> (<code>torch.Tensor</code>, 可选): 预计算的提示嵌入</li>
<li><strong><code>prompt_embeds_mask</code></strong> (<code>torch.Tensor</code>, 可选): 提示嵌入的掩码</li>
<li><strong><code>negative_prompt_embeds</code></strong> (<code>torch.Tensor</code>, 可选): 负面提示嵌入</li>
<li><strong><code>negative_prompt_embeds_mask</code></strong> (<code>torch.Tensor</code>, 可选): 负面提示嵌入掩码</li>
</ul>
<h3 id="%E8%BE%93%E5%87%BA%E6%8E%A7%E5%88%B6">输出控制</h3>
<ul>
<li><strong><code>output_type</code></strong> (<code>str</code>, 默认 &quot;pil&quot;): 输出格式</li>
<li><strong><code>return_dict</code></strong> (<code>bool</code>, 默认 True): 是否返回字典格式结果</li>
</ul>
<h3 id="%E5%9B%9E%E8%B0%83%E5%92%8C%E8%B0%83%E8%AF%95">回调和调试</h3>
<ul>
<li><strong><code>callback_on_step_end</code></strong> (<code>Callable</code>, 可选): 每步结束时的回调函数</li>
<li><strong><code>callback_on_step_end_tensor_inputs</code></strong> (<code>List[str]</code>): 传递给回调函数的张量名称</li>
<li><strong><code>attention_kwargs</code></strong> (<code>Dict</code>, 可选): 注意力机制的额外参数</li>
<li><strong><code>max_sequence_length</code></strong> (<code>int</code>, 默认 512): 最大序列长度</li>
</ul>
<h2 id="%E8%BF%94%E5%9B%9E%E5%80%BC">返回值</h2>
<h3 id="qwenimagepipelineoutput">QwenImagePipelineOutput</h3>
<p>当 <code>return_dict=True</code> 时返回 <code>QwenImagePipelineOutput</code> 对象：</p>
<pre class="hljs"><code><div><span class="hljs-meta">@dataclass</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QwenImagePipelineOutput</span><span class="hljs-params">(BaseOutput)</span>:</span>
    images: Union[List[PIL.Image.Image], np.ndarray]
</div></code></pre>
<h3 id="tuple">Tuple</h3>
<p>当 <code>return_dict=False</code> 时返回元组：</p>
<pre class="hljs"><code><div>(images,)  <span class="hljs-comment"># 第一个元素是图像列表</span>
</div></code></pre>
<h2 id="%E5%86%85%E9%83%A8%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B">内部处理流程</h2>
<pre><code class="language-mermaid"><div class="mermaid">flowchart TD
    A[开始: __call__ 方法调用] --> B[1. 输入预处理]
    B --> B1[验证输入参数]
    B1 --> B2[计算图像尺寸]
    B2 --> B3[设置批次大小]
    
    B3 --> C[2. 图像预处理]
    C --> C1[预处理输入图像]
    C1 --> C2[调整图像尺寸]
    C2 --> C3[图像格式转换]
    
    C3 --> D[3. 文本编码]
    D --> D1[编码正面提示]
    D1 --> D2{是否有负面提示?}
    D2 -->|是| D3[编码负面提示]
    D2 -->|否| E[4. 潜在空间准备]
    D3 --> E
    
    E --> E1[VAE编码输入图像]
    E1 --> E2[根据强度添加噪声]
    E2 --> E3[准备初始潜在表示]
    E3 --> E4[准备图像形状信息]
    
    E4 --> F[5. 时间步调度]
    F --> F1[计算噪声调度参数]
    F1 --> F2[根据强度调整起始时间步]
    F2 --> F3[生成时间步序列]
    F3 --> F4[计算动态偏移参数]
    
    F4 --> G[6. 去噪循环开始]
    G --> G1{遍历时间步}
    G1 --> G2[Transformer预测噪声]
    G2 --> G3{是否使用CFG?}
    G3 -->|是| G4[计算负面噪声预测]
    G3 -->|否| G5[应用调度器更新]
    G4 --> G6[应用CFG组合]
    G6 --> G5
    G5 --> G7[更新潜在表示]
    G7 --> G8{是否有回调?}
    G8 -->|是| G9[执行回调函数]
    G8 -->|否| G10{是否完成所有步骤?}
    G9 --> G10
    G10 -->|否| G1
    G10 -->|是| H[7. 后处理和输出]
    
    H --> H1{输出类型检查}
    H1 -->|latent| H2[直接返回潜在表示]
    H1 -->|其他| H3[VAE解码潜在表示]
    H3 --> H4[图像后处理]
    H4 --> H5[格式转换]
    H2 --> I[8. 返回结果]
    H5 --> I
    
    I --> I1{return_dict?}
    I1 -->|True| I2[返回QwenImagePipelineOutput]
    I1 -->|False| I3[返回元组]
    I2 --> J[结束]
    I3 --> J
    
    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style G fill:#fff3e0
    style H fill:#f3e5f5
    style E fill:#e8f5e8
    style F2 fill:#fff9c4
</div></code></pre>
<h2 id="%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82">技术细节</h2>
<h3 id="%E5%BC%BA%E5%BA%A6%E6%8E%A7%E5%88%B6%E6%9C%BA%E5%88%B6">强度控制机制</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 根据强度计算起始时间步</span>
init_timestep = min(int(num_inference_steps * strength), num_inference_steps)
t_start = max(num_inference_steps - init_timestep, <span class="hljs-number">0</span>)
timesteps = timesteps[t_start * self.scheduler.order :]

<span class="hljs-comment"># 为输入图像添加噪声</span>
<span class="hljs-keyword">if</span> latents <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
    noise = randn_tensor(init_latents.shape, generator=generator, device=device, dtype=init_latents.dtype)
    init_latents = self.scheduler.scale_noise(init_latents, timesteps[:<span class="hljs-number">1</span>], noise)
    latents = init_latents
</div></code></pre>
<h3 id="%E5%9B%BE%E5%83%8F%E7%BC%96%E7%A0%81%E8%BF%87%E7%A8%8B">图像编码过程</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># VAE 编码输入图像</span>
init_image = self.image_processor.preprocess(image, height=height, width=width)
init_image = init_image.to(dtype=torch.float32)

<span class="hljs-comment"># 编码到潜在空间</span>
init_latents = retrieve_latents(self.vae.encode(init_image), generator=generator)
init_latents = (init_latents - latents_mean) * latents_std
init_latents = init_latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)

<span class="hljs-comment"># 打包潜在表示</span>
init_latents = self._pack_latents(
    init_latents,
    batch_size=init_latents.shape[<span class="hljs-number">0</span>],
    num_channels_latents=num_channels_latents,
    height=init_latents.shape[<span class="hljs-number">3</span>],
    width=init_latents.shape[<span class="hljs-number">4</span>],
)
</div></code></pre>
<h3 id="%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF">提示模板</h3>
<p>QwenImageImg2ImgPipeline 使用图像描述模板：</p>
<pre class="hljs"><code><div>template = <span class="hljs-string">"&lt;|im_start|&gt;system\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"</span>
</div></code></pre>
<h2 id="%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B">使用示例</h2>
<h3 id="%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2">基本用法（风格转换）</h3>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> QwenImageImg2ImgPipeline
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-comment"># 加载管道</span>
pipeline = QwenImageImg2ImgPipeline.from_pretrained(
    <span class="hljs-string">"Qwen/Qwen-Image"</span>, 
    torch_dtype=torch.bfloat16
)
pipeline.to(<span class="hljs-string">"cuda"</span>)

<span class="hljs-comment"># 加载输入图像</span>
input_image = Image.open(<span class="hljs-string">"mountain_sketch.jpg"</span>).resize((<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>))

<span class="hljs-comment"># 执行图像转换</span>
result = pipeline(
    prompt=<span class="hljs-string">"猫咪巫师，甘道夫，指环王，详细，奇幻，可爱，皮克斯，迪士尼"</span>,
    negative_prompt=<span class="hljs-string">"模糊，低质量，扭曲"</span>,
    image=input_image,
    strength=<span class="hljs-number">0.95</span>,  <span class="hljs-comment"># 高强度转换</span>
    num_inference_steps=<span class="hljs-number">50</span>,
    true_cfg_scale=<span class="hljs-number">4.0</span>
)

<span class="hljs-comment"># 保存结果</span>
transformed_image = result.images[<span class="hljs-number">0</span>]
transformed_image.save(<span class="hljs-string">"img2img_output.jpg"</span>)
</div></code></pre>
<h3 id="%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%94%9F%E6%88%90">高级用法（多样化生成）</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 生成多个变体</span>
result = pipeline(
    prompt=<span class="hljs-string">"现代艺术风格，抽象表现主义，色彩丰富"</span>,
    negative_prompt=<span class="hljs-string">"写实，照片，单调"</span>,
    image=input_image,
    strength=<span class="hljs-number">0.7</span>,
    num_images_per_prompt=<span class="hljs-number">4</span>,  <span class="hljs-comment"># 生成4个变体</span>
    num_inference_steps=<span class="hljs-number">75</span>,
    true_cfg_scale=<span class="hljs-number">5.0</span>,
    generator=torch.manual_seed(<span class="hljs-number">42</span>)
)

<span class="hljs-comment"># 保存所有变体</span>
<span class="hljs-keyword">for</span> i, img <span class="hljs-keyword">in</span> enumerate(result.images):
    img.save(<span class="hljs-string">f"variant_<span class="hljs-subst">{i}</span>.jpg"</span>)
</div></code></pre>
<h3 id="%E6%89%B9%E9%87%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2">批量风格转换</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 不同风格的批量转换</span>
styles = [
    <span class="hljs-string">"水彩画风格，柔和色彩"</span>,
    <span class="hljs-string">"油画风格，厚重笔触"</span>,
    <span class="hljs-string">"动漫风格，鲜艳色彩"</span>,
    <span class="hljs-string">"素描风格，黑白线条"</span>
]

results = []
<span class="hljs-keyword">for</span> style <span class="hljs-keyword">in</span> styles:
    result = pipeline(
        prompt=style,
        negative_prompt=<span class="hljs-string">"模糊，低质量"</span>,
        image=input_image,
        strength=<span class="hljs-number">0.8</span>,
        num_inference_steps=<span class="hljs-number">50</span>
    )
    results.extend(result.images)

<span class="hljs-comment"># 保存所有风格</span>
<span class="hljs-keyword">for</span> i, img <span class="hljs-keyword">in</span> enumerate(results):
    img.save(<span class="hljs-string">f"style_<span class="hljs-subst">{i}</span>.jpg"</span>)
</div></code></pre>
<h3 id="%E6%B8%90%E8%BF%9B%E5%BC%8F%E8%BD%AC%E6%8D%A2">渐进式转换</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 不同强度的渐进转换</span>
strengths = [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.9</span>]

<span class="hljs-keyword">for</span> i, strength <span class="hljs-keyword">in</span> enumerate(strengths):
    result = pipeline(
        prompt=<span class="hljs-string">"梵高风格，星夜，旋涡状笔触"</span>,
        negative_prompt=<span class="hljs-string">"现代，数字艺术"</span>,
        image=input_image,
        strength=strength,
        num_inference_steps=<span class="hljs-number">50</span>,
        true_cfg_scale=<span class="hljs-number">4.0</span>
    )
    result.images[<span class="hljs-number">0</span>].save(<span class="hljs-string">f"strength_<span class="hljs-subst">{strength}</span>.jpg"</span>)
</div></code></pre>
<h2 id="%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE">性能优化建议</h2>
<ol>
<li>
<p><strong>强度设置</strong>:</p>
<ul>
<li>轻微修改: 0.2-0.4</li>
<li>风格转换: 0.6-0.8</li>
<li>大幅变换: 0.8-0.95</li>
</ul>
</li>
<li>
<p><strong>推理步数</strong>:</p>
<ul>
<li>快速预览: 25-30 步</li>
<li>标准质量: 50 步</li>
<li>高质量: 75-100 步</li>
</ul>
</li>
<li>
<p><strong>CFG 缩放</strong>:</p>
<ul>
<li>保守转换: 3.0-4.0</li>
<li>标准转换: 4.0-6.0</li>
<li>激进转换: 6.0-8.0</li>
</ul>
</li>
<li>
<p><strong>内存优化</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 启用内存优化</span>
pipeline.enable_model_cpu_offload()
pipeline.enable_attention_slicing()
</div></code></pre>
</li>
<li>
<p><strong>批处理优化</strong>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 批量处理多个图像</span>
images = [img1, img2, img3]
result = pipeline(
    prompt=<span class="hljs-string">"统一风格转换"</span>,
    image=images,
    strength=<span class="hljs-number">0.7</span>
)
</div></code></pre>
</li>
</ol>
<h2 id="%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">常见问题和解决方案</h2>
<h3 id="1-%E8%BD%AC%E6%8D%A2%E6%95%88%E6%9E%9C%E4%B8%8D%E6%98%8E%E6%98%BE">1. 转换效果不明显</h3>
<ul>
<li>增加 <code>strength</code> 值到 0.8-0.9</li>
<li>提高 <code>true_cfg_scale</code> 到 6.0-8.0</li>
<li>使用更具体的风格描述</li>
</ul>
<h3 id="2-%E4%B8%A2%E5%A4%B1%E5%8E%9F%E5%9B%BE%E9%87%8D%E8%A6%81%E7%BB%86%E8%8A%82">2. 丢失原图重要细节</h3>
<ul>
<li>降低 <code>strength</code> 值到 0.4-0.6</li>
<li>在提示中描述要保留的元素</li>
<li>使用更精确的负面提示</li>
</ul>
<h3 id="3-%E7%94%9F%E6%88%90%E7%BB%93%E6%9E%9C%E4%B8%8D%E7%A8%B3%E5%AE%9A">3. 生成结果不稳定</h3>
<ul>
<li>固定随机种子: <code>generator=torch.manual_seed(42)</code></li>
<li>增加推理步数到 75-100</li>
<li>调整 CFG 缩放到合适范围</li>
</ul>
<h3 id="4-%E5%A4%84%E7%90%86%E9%80%9F%E5%BA%A6%E6%85%A2">4. 处理速度慢</h3>
<ul>
<li>减少推理步数到 25-30</li>
<li>使用较小的图像尺寸</li>
<li>启用模型 CPU 卸载</li>
</ul>
<h2 id="%E4%B8%8E%E5%85%B6%E4%BB%96-pipeline-%E7%9A%84%E5%8C%BA%E5%88%AB">与其他 Pipeline 的区别</h2>
<table>
<thead>
<tr>
<th>特性</th>
<th>QwenImageImg2ImgPipeline</th>
<th>StableDiffusionImg2ImgPipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本编码器</td>
<td>Qwen2.5-VL (多模态)</td>
<td>CLIP (纯文本)</td>
</tr>
<tr>
<td>图像理解</td>
<td>原生支持</td>
<td>需要额外处理</td>
</tr>
<tr>
<td>风格转换质量</td>
<td>高（理解图像内容）</td>
<td>中等</td>
</tr>
<tr>
<td>细节保留</td>
<td>智能保留</td>
<td>标准保留</td>
</tr>
<tr>
<td>提示模板</td>
<td>专用描述模板</td>
<td>通用模板</td>
</tr>
<tr>
<td>多模态能力</td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody>
</table>
<h2 id="%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">应用场景</h2>
<ol>
<li><strong>艺术风格转换</strong>: 将照片转换为绘画风格</li>
<li><strong>概念设计</strong>: 将草图转换为精细设计</li>
<li><strong>图像增强</strong>: 提升图像质量和细节</li>
<li><strong>创意变换</strong>: 改变图像的主题或风格</li>
<li><strong>批量处理</strong>: 统一多张图像的风格</li>
</ol>

</body>
</html>
